# 模型微調策略筆記 (Fine-tuning Strategy Notes)

針對 Ollama/Llama 模型微調 (SFT) 的策略整理。目前的訓練方式屬於**強監督式微調 (Supervised Fine-Tuning, SFT)**，若資料過於單一，容易導致模型變成死板的「問答機器」。

以下整理三種能有效左右訓練結果、提升模型靈活度與邏輯能力的介入層面。

## 1. 資料層面 (Data Layer)

這是影響力最大的層面，約決定 80% 的最終效果。目標是讓模型學習「意圖」而非單純的「關鍵字匹配」。

### 增加多樣性 (Diversity & Variance)
*   **問題**：只使用固定模板 (如 `請問{topic}是什麼？`) 會限制模型的理解能力。
*   **策略**：
    *   引入更多變的問法（口語化、簡潔句、甚至包含錯字）。
    *   使用 LLM 協助重寫訓練資料，生成 50-100 種不同的問句變體。

### 加入 System Prompt (人設與指示)
*   **問題**：僅使用 `user` 和 `assistant` 角色，模型缺乏明確的性格或回答準則。
*   **策略**：
    *   在資料中加入 `system` 角色。
    *   範例：`{"role": "system", "content": "你是一位說話刻薄但專業的星際列車站務員..."}`。
    *   **效果**：模型會內化這種語氣和態度，無需在每次回答中刻意教導。

### 思維鏈 (Chain of Thought, CoT)
*   **問題**：直接給出答案，模型只能死記硬背。
*   **策略**：
    *   修改 `assistant` 的回答，加入推論過程。
    *   範例：`思考過後，根據星際規章第42條...考量到安全性...所以答案是...`。
    *   **效果**：提升模型面對未知問題時的舉一反三能力。

---

## 2. 訓練參數層面 (Hyperparameters)

調整這些參數可以改變模型學習的「深度」與「泛化能力」。

### LoRA 參數 (`r` & `lora_alpha`)
*   **`r` (Rank)**：LoRA 的秩。
    *   **調大 (64, 128)**：增加模型參數容量，能學會更多複雜的新知識（如大量法規細節），但風險是容易過擬合 (Overfitting)。
    *   **調小 (8, 16)**：適合簡單任務或風格調整。
*   **`lora_alpha`**：縮放係數 (Scaling Factor)。
    *   通常設為 `r` 的 2 倍。
    *   **調高**：讓模型更強烈地表現出微調後的特徵。
    *   **調低**：保留更多原模型的通用能力。

### 噪聲嵌入 (NEFTune)
*   **策略**：在 `SFTConfig` 中加入 `neftune_noise_alpha=5` (數值可微調)。
*   **原理**：在 Embedding 層加入噪聲。
*   **效果**：強迫模型不要死記硬背輸入數據的數值特徵，而是學習分佈。**對於小資料集 (Small Datasets) 特別有效**，能顯著提升對話的泛化能力。

### 學習率與 Epochs
*   **策略**：若發現模型開始死背，嘗試**降低學習率 (如 5e-5)** 或 **減少 Epoch**。

---

## 3. 資料結構層面 (Structure)

改變對話的編排方式，教導模型理解上下文與邊界。

### 多輪對話 (Multi-turn)
*   **問題**：單輪問答無法訓練上下文理解。
*   **策略**：
    *   建構 `user` -> `assistant` -> `user` -> `assistant` 的連續對話資料。
    *   **效果**：讓模型學會理解代名詞（如「那如果不這樣做呢？」中的「那」）與對話脈絡。

### 負樣本 (Negative Samples)
*   **問題**：模型容易產生幻覺，強行回答不相關的問題。
*   **策略**：
    *   加入與領域無關的問題。
    *   將答案設為拒絕回答（例如：「這不在星際鐵道便當管理局的管轄範圍內。」）。
    *   **效果**：明確界定模型的能力邊界，減少幻覺 (Hallucination)。
